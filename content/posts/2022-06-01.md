---
title: "2022 06 01"
date: 2023-06-01T13:02:27-05:00
draft: true
---

## Stuff to read if you are bored:
1.  Training Large Language models for intermediate steps increases its capabilities, (in addition with an agressive math token dataset to train it).
    - OpenAI released a paper demonstrating a significant improvement in ChatGPT's performance in mathematics and other domains.
    - They trained two reward models for GPT-4: one for final answers and another for intermediate reasoning steps.
    - Rewarding good working out (reasoning steps) led to solving 78% of math problems, nearly doubling GPT-4's raw performance.
    - Rewarding good working out outperformed rewarding correct answers only and surpassed previous state-of-the-art performance.
    - The approach of process supervision has implications beyond mathematics, showing promise in calculus, chemistry, physics, and other scientific domains.
    - https://cdn.openai.com/improving-mathematical-reasoning-with-process-supervision/Lets_Verify_Step_by_Step.pdf
2. ChatGPT code interpreter plugin: data analytics, visualzations, file uploading, file format conversion, complex math computation and more.
    - https://www.nftparis.xyz/blog/chatgpts-code-interpreter-the-future-of-ai-powered-coding#:~:text=The%20Code%20Interpreter%20plugin%20is,perform%20file%20conversions%20with%20ease.

3. What is a Library economy? -- software i think would would be cool to help people connect to share time and items together to foster community.
    - fosters communities through sharing items with eachother and coordinating neighborhood labor to help members in the community out or participate in group community projects.
    - https://libraryeconomy.nyc/
